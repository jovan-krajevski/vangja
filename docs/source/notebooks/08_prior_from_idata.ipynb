{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 08: Transfer Learning with `prior_from_idata` and Hierarchical Modeling\n",
    "\n",
    "In the previous chapter, we demonstrated how to improve short time series forecasting by transferring learned seasonality from a long related time series using `tune_method=\"parametric\"`. That approach extracts the posterior **mean and standard deviation** from the source model and uses them as the parameters of a new Normal prior.\n",
    "\n",
    "In this chapter, we explore a more powerful alternative: `tune_method=\"prior_from_idata\"`. Instead of summarizing the posterior into two numbers (mean, std), this method uses the **full posterior distribution** from the source model as the prior for the target model — preserving correlations between parameters and any non-Gaussian structure in the posterior.\n",
    "\n",
    "We also demonstrate how to **combine hierarchical modeling with transfer learning**: fitting a single joint model over both the long temperature series and the short bike sales series, with partial pooling on the yearly seasonality informed by the temperature model's posterior. This is one of vangja's most powerful features for short time series forecasting.\n",
    "\n",
    "> **Reference**: The `prior_from_idata` functionality is provided by [pymc-extras](https://www.pymc.io/projects/extras/en/stable/generated/pymc_extras.utils.prior.prior_from_idata.html). It was inspired by the [Updating Priors](https://www.pymc.io/projects/examples/en/latest/howto/updating_priors.html) example in PyMC, which demonstrates how to iteratively update priors with posterior knowledge from previous analyses.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll cover three approaches in this notebook:\n",
    "1. **Recap**: Fit the temperature model to learn yearly seasonality (same as Chapter 07)\n",
    "2. **`prior_from_idata` transfer**: Transfer the full posterior to the bike sales model\n",
    "3. **Hierarchical + transfer learning**: Fit a joint model over both series with partial pooling and informed priors — the most powerful approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from vangja import FlatTrend, FourierSeasonality\n",
    "from vangja.datasets import load_citi_bike_sales, load_nyc_temperature\n",
    "from vangja.utils import metrics\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Load Data and Fit the Temperature Model\n",
    "\n",
    "This step is identical to Chapter 07. We load the NYC temperature and Citi Bike sales datasets, create the short training set, and fit a temperature model using MCMC to learn yearly seasonality. The MCMC posterior will then be used in the transfer learning steps that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "temp_df = load_nyc_temperature()\n",
    "sales_df = load_citi_bike_sales()\n",
    "\n",
    "# Match the temperature data to the sales date range (as in the original Tim Radtke blog post)\n",
    "temp_df = temp_df[\n",
    "    (temp_df[\"ds\"] >= sales_df[\"ds\"].min()) & (temp_df[\"ds\"] <= sales_df[\"ds\"].max())\n",
    "]\n",
    "\n",
    "print(\"Temperature data:\")\n",
    "print(f\"  Shape: {temp_df.shape}\")\n",
    "print(f\"  Date range: {temp_df['ds'].min().date()} to {temp_df['ds'].max().date()}\")\n",
    "\n",
    "print(\"\\nSales data:\")\n",
    "print(f\"  Shape: {sales_df.shape}\")\n",
    "print(f\"  Date range: {sales_df['ds'].min().date()} to {sales_df['ds'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split: use only the first ~3 months for training\n",
    "train_test_date = pd.to_datetime(\"2013-10-15\")\n",
    "\n",
    "sales_train = sales_df[sales_df[\"ds\"] < train_test_date].copy()\n",
    "sales_test = sales_df[sales_df[\"ds\"] >= train_test_date].copy()\n",
    "\n",
    "print(\n",
    "    f\"Training period: {sales_train['ds'].min().date()} to {sales_train['ds'].max().date()}\"\n",
    ")\n",
    "print(f\"Training samples: {len(sales_train)} days\")\n",
    "print(\n",
    "    f\"\\nTest period: {sales_test['ds'].min().date()} to {sales_test['ds'].max().date()}\"\n",
    ")\n",
    "print(f\"Test samples: {len(sales_test)} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both datasets and the train/test split\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Temperature\n",
    "axes[0].plot(temp_df[\"ds\"], temp_df[\"y\"], \"C0-\", linewidth=0.5, alpha=0.7)\n",
    "axes[0].set_title(\"NYC Daily Max Temperature (Long Series)\")\n",
    "axes[0].set_ylabel(\"Temperature (°F)\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sales with train/test split\n",
    "axes[1].plot(\n",
    "    sales_train[\"ds\"],\n",
    "    sales_train[\"y\"],\n",
    "    \"C0o-\",\n",
    "    markersize=2,\n",
    "    linewidth=0.5,\n",
    "    alpha=0.7,\n",
    "    label=\"Training\",\n",
    ")\n",
    "axes[1].plot(\n",
    "    sales_test[\"ds\"],\n",
    "    sales_test[\"y\"],\n",
    "    \"C1o-\",\n",
    "    markersize=2,\n",
    "    linewidth=0.5,\n",
    "    alpha=0.7,\n",
    "    label=\"Test (holdout)\",\n",
    ")\n",
    "axes[1].axvline(\n",
    "    train_test_date, color=\"gray\", linestyle=\"--\", linewidth=2, label=\"Train/Test Split\"\n",
    ")\n",
    "axes[1].set_title(\"Citi Bike Station 360 Daily Sales\")\n",
    "axes[1].set_ylabel(\"Number of Sales\")\n",
    "axes[1].set_xlabel(\"Date\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Temperature Model with MCMC\n",
    "\n",
    "We use a simple `FlatTrend + FourierSeasonality` model and fit it with NUTS (MCMC) to get full posterior samples. These samples will serve as the basis for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature model: flat trend + yearly seasonality\n",
    "temp_model = FlatTrend() + FourierSeasonality(period=365.25, series_order=6)\n",
    "\n",
    "print(\"Fitting temperature model with NUTS...\")\n",
    "temp_model.fit(temp_df, method=\"nuts\", scaler=\"minmax\")\n",
    "\n",
    "# Get predictions to visualize the fit\n",
    "temp_pred = temp_model.predict(horizon=0, freq=\"D\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature model fit\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "ax.plot(temp_df[\"ds\"], temp_df[\"y\"], \"C0.\", markersize=1, label=\"Temperature data\")\n",
    "ax.plot(temp_pred[\"ds\"], temp_pred[\"yhat_0\"], \"r-\", linewidth=1.5, label=\"Model fit\")\n",
    "\n",
    "ax.set_title(\"Temperature Model Fit (NUTS)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Temperature (°F)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component decomposition\n",
    "temp_model.plot(temp_pred, y_true=temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Transfer Learning with `prior_from_idata`\n",
    "\n",
    "### What is `prior_from_idata`?\n",
    "\n",
    "In Chapter 07, we used `tune_method=\"parametric\"`, which transfers knowledge by:\n",
    "1. Computing the **mean** and **standard deviation** of each posterior parameter\n",
    "2. Using these as the parameters of an independent Normal prior: $\\beta_i^{\\text{new}} \\sim \\text{Normal}(\\mu_i, \\sigma_i)$\n",
    "\n",
    "This is simple and effective, but it **discards** two important aspects of the posterior:\n",
    "- **Correlations** between parameters (e.g., the sine and cosine terms at the same frequency are often correlated)\n",
    "- **Non-Gaussian structure** (e.g., skewness, multimodality)\n",
    "\n",
    "The `tune_method=\"prior_from_idata\"` approach instead uses the [prior_from_idata](https://www.pymc.io/projects/extras/en/stable/generated/pymc_extras.utils.prior.prior_from_idata.html) function from **pymc-extras**, which:\n",
    "1. Takes the full MCMC posterior samples from the source model\n",
    "2. Fits a **multivariate Normal distribution** to the posterior samples\n",
    "3. Creates a PyMC random variable from this distribution that can serve as the prior in the new model\n",
    "\n",
    "This means the new model's prior is a **multivariate Normal approximation** of the source model's full posterior — preserving the covariance structure between all parameters.\n",
    "\n",
    "Mathematically, instead of:\n",
    "\n",
    "$$\\beta_i^{\\text{new}} \\sim \\text{Normal}(\\mu_i, \\sigma_i) \\quad \\text{(independent)}$$\n",
    "\n",
    "we get:\n",
    "\n",
    "$$\\boldsymbol{\\beta}^{\\text{new}} \\sim \\text{MultivariateNormal}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$$\n",
    "\n",
    "where $\\boldsymbol{\\mu}$ is the posterior mean vector and $\\boldsymbol{\\Sigma}$ is the full posterior covariance matrix.\n",
    "\n",
    "### When to use `prior_from_idata` vs `parametric`\n",
    "\n",
    "| Aspect | `parametric` | `prior_from_idata` |\n",
    "|--------|-------------|-------------------|\n",
    "| **What it transfers** | Mean & std per parameter | Full covariance structure |\n",
    "| **Prior shape** | Independent Normal | Multivariate Normal |\n",
    "| **Correlations** | ❌ Discarded | ✅ Preserved |\n",
    "| **Complexity** | Simple | More complex |\n",
    "| **Inference method** | Works with MAP, ADVI, NUTS | Works with MAP, ADVI, NUTS |\n",
    "| **Source model** | MCMC or VI required | MCMC or VI required |\n",
    "| **Best for** | Quick prototyping, when correlations don't matter | When parameter correlations are important |\n",
    "\n",
    "For Fourier seasonality transfer, `prior_from_idata` is typically the better choice because the sine and cosine coefficients at each frequency **are** correlated — they jointly determine the phase and amplitude of each harmonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the correlation structure in the temperature posterior\n",
    "# to understand why prior_from_idata matters\n",
    "\n",
    "beta_key = \"fs_0 - beta(p=365.25,n=6)\"\n",
    "beta_posterior = temp_model.trace[\"posterior\"][beta_key]\n",
    "\n",
    "# Reshape to (n_samples, n_coefficients)\n",
    "beta_samples = beta_posterior.values.reshape(-1, 12)\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = np.corrcoef(beta_samples.T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "labels = [f\"{'sin' if i % 2 == 0 else 'cos'}({i // 2 + 1})\" for i in range(12)]\n",
    "im = ax.imshow(corr_matrix, cmap=\"RdBu_r\", vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(12))\n",
    "ax.set_yticks(range(12))\n",
    "ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_title(\n",
    "    \"Posterior Correlation Matrix of Fourier Coefficients\\n(from Temperature Model)\"\n",
    ")\n",
    "plt.colorbar(im, ax=ax, label=\"Correlation\")\n",
    "\n",
    "# Annotate with correlation values\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        ax.text(\n",
    "            j,\n",
    "            i,\n",
    "            f\"{corr_matrix[i, j]:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=7,\n",
    "            color=\"white\" if abs(corr_matrix[i, j]) > 0.5 else \"black\",\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Non-zero off-diagonal correlations show that parametric transfer\")\n",
    "print(\"(which assumes independence) discards valuable information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Sales Model with `prior_from_idata`\n",
    "\n",
    "Now we create and fit a sales model that uses `tune_method=\"prior_from_idata\"` for the yearly seasonality component. Under the hood, vangja calls `pymc_extras.utils.prior.prior_from_idata()` to build a multivariate Normal approximation of the temperature model's posterior and uses it as the prior for the sales model's Fourier coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning model using prior_from_idata\n",
    "transfer_pfi_model = (\n",
    "    FlatTrend()\n",
    "    + FourierSeasonality(\n",
    "        period=365.25,\n",
    "        series_order=6,\n",
    "        tune_method=\"prior_from_idata\",  # Use full posterior as prior\n",
    "    )\n",
    "    + FourierSeasonality(period=91.31, series_order=4)  # Quarterly\n",
    "    + FourierSeasonality(period=30.44, series_order=3)  # Monthly\n",
    "    + FourierSeasonality(period=7, series_order=3)  # Weekly\n",
    ")\n",
    "\n",
    "print(\"Fitting transfer learning model (prior_from_idata)...\")\n",
    "print(\"  - Yearly seasonality: full posterior transferred from temperature model\")\n",
    "print(\"  - Other components: learned from sales data with default priors\")\n",
    "\n",
    "transfer_pfi_model.fit(\n",
    "    sales_train,\n",
    "    method=\"mapx\",\n",
    "    idata=temp_model.trace,\n",
    "    t_scale_params=temp_model.t_scale_params,\n",
    "    scaler=\"minmax\",\n",
    ")\n",
    "\n",
    "transfer_pfi_pred = transfer_pfi_model.predict(horizon=len(sales_test), freq=\"D\")\n",
    "transfer_pfi_pred = transfer_pfi_pred[\n",
    "    (transfer_pfi_pred[\"ds\"] >= sales_train[\"ds\"].min())\n",
    "    & (transfer_pfi_pred[\"ds\"] <= sales_test[\"ds\"].max())\n",
    "]\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare `prior_from_idata` with `parametric`\n",
    "\n",
    "Let's also fit the `parametric` version (same as Chapter 07) so we can compare both approaches directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison: the parametric transfer model (same as Chapter 07)\n",
    "transfer_param_model = (\n",
    "    FlatTrend()\n",
    "    + FourierSeasonality(\n",
    "        period=365.25,\n",
    "        series_order=6,\n",
    "        tune_method=\"parametric\",  # Mean/std transfer only\n",
    "    )\n",
    "    + FourierSeasonality(period=91.31, series_order=4)\n",
    "    + FourierSeasonality(period=30.44, series_order=3)\n",
    "    + FourierSeasonality(period=7, series_order=3)\n",
    ")\n",
    "\n",
    "print(\"Fitting transfer learning model (parametric)...\")\n",
    "transfer_param_model.fit(\n",
    "    sales_train,\n",
    "    method=\"mapx\",\n",
    "    idata=temp_model.trace,\n",
    "    t_scale_params=temp_model.t_scale_params,\n",
    "    scaler=\"minmax\",\n",
    ")\n",
    "\n",
    "transfer_param_pred = transfer_param_model.predict(horizon=len(sales_test), freq=\"D\")\n",
    "transfer_param_pred = transfer_param_pred[\n",
    "    (transfer_param_pred[\"ds\"] >= sales_train[\"ds\"].min())\n",
    "    & (transfer_param_pred[\"ds\"] <= sales_test[\"ds\"].max())\n",
    "]\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison: parametric vs prior_from_idata\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "for ax, (model_name, pred, color) in zip(\n",
    "    axes,\n",
    "    [\n",
    "        (\"Parametric Transfer (mean/std)\", transfer_param_pred, \"C2\"),\n",
    "        (\"prior_from_idata Transfer (full posterior)\", transfer_pfi_pred, \"C4\"),\n",
    "    ],\n",
    "):\n",
    "    ax.plot(\n",
    "        sales_train[\"ds\"],\n",
    "        sales_train[\"y\"],\n",
    "        \"C0o\",\n",
    "        markersize=3,\n",
    "        alpha=0.5,\n",
    "        label=\"Training data\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        sales_test[\"ds\"],\n",
    "        sales_test[\"y\"],\n",
    "        \"C1o\",\n",
    "        markersize=3,\n",
    "        alpha=0.5,\n",
    "        label=\"Test data (actual)\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        pred[\"ds\"],\n",
    "        pred[\"yhat_0\"],\n",
    "        f\"{color}-\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.8,\n",
    "        label=f\"{model_name} prediction\",\n",
    "    )\n",
    "    ax.axvline(train_test_date, color=\"gray\", linestyle=\"--\", linewidth=2)\n",
    "    ax.set_title(f\"{model_name}\")\n",
    "    ax.set_ylabel(\"Number of Sales\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel(\"Date\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay both transfer learning approaches\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(\n",
    "    sales_train[\"ds\"],\n",
    "    sales_train[\"y\"],\n",
    "    \"C0o\",\n",
    "    markersize=3,\n",
    "    alpha=0.4,\n",
    "    label=\"Training data\",\n",
    ")\n",
    "ax.plot(\n",
    "    sales_test[\"ds\"],\n",
    "    sales_test[\"y\"],\n",
    "    \"C1o\",\n",
    "    markersize=3,\n",
    "    alpha=0.4,\n",
    "    label=\"Test data (actual)\",\n",
    ")\n",
    "ax.plot(\n",
    "    transfer_param_pred[\"ds\"],\n",
    "    transfer_param_pred[\"yhat_0\"],\n",
    "    \"C2-\",\n",
    "    linewidth=1.5,\n",
    "    alpha=0.8,\n",
    "    label=\"Parametric\",\n",
    ")\n",
    "ax.plot(\n",
    "    transfer_pfi_pred[\"ds\"],\n",
    "    transfer_pfi_pred[\"yhat_0\"],\n",
    "    \"C4-\",\n",
    "    linewidth=1.5,\n",
    "    alpha=0.8,\n",
    "    label=\"prior_from_idata\",\n",
    ")\n",
    "\n",
    "ax.axvline(\n",
    "    train_test_date, color=\"gray\", linestyle=\"--\", linewidth=2, label=\"Train/Test Split\"\n",
    ")\n",
    "ax.set_title(\"Transfer Learning Comparison: parametric vs prior_from_idata\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Sales\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative comparison\n",
    "param_metrics = metrics(sales_test, transfer_param_pred, pool_type=\"complete\")\n",
    "pfi_metrics = metrics(sales_test, transfer_pfi_pred, pool_type=\"complete\")\n",
    "\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Parametric\": param_metrics.iloc[0],\n",
    "        \"prior_from_idata\": pfi_metrics.iloc[0],\n",
    "    },\n",
    ").T\n",
    "\n",
    "print(\"Test Set Metrics: parametric vs prior_from_idata\")\n",
    "print(\"=\" * 55)\n",
    "display(comparison_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component decomposition for the prior_from_idata model\n",
    "transfer_pfi_model.plot(transfer_pfi_pred, y_true=sales_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Combining Hierarchical Modeling with Transfer Learning\n",
    "\n",
    "Now we demonstrate the most powerful approach in vangja: **fitting a hierarchical model with partial pooling over both the temperature and bike sales series, while also informing the shared yearly seasonality prior with the temperature model's posterior**.\n",
    "\n",
    "### Why is this powerful?\n",
    "\n",
    "In the previous step, we transferred knowledge from temperature to bike sales in a **one-to-one** fashion. But consider the general case: you might have:\n",
    "- One long \"source\" series (e.g., temperature with 3+ years of data)\n",
    "- Multiple short \"target\" series (e.g., bike sales from different stations, each with only a few months)\n",
    "\n",
    "Hierarchical modeling with partial pooling lets you:\n",
    "1. **Share seasonal information** across all series through a shared hyperprior\n",
    "2. **Inform the shared prior** with external knowledge via transfer learning\n",
    "3. **Allow individual deviations** — each series can still have its own seasonal amplitude and phase\n",
    "\n",
    "The result is a model where:\n",
    "- The shared yearly seasonality pattern is **warm-started** from the temperature posterior\n",
    "- **All series borrow strength** from each other through partial pooling\n",
    "- The temperature series (with years of data) provides a strong anchor for the seasonal shape\n",
    "- The short bike sales series can deviate from the shared pattern where the data supports it\n",
    "\n",
    "### Setting up the hierarchical dataset\n",
    "\n",
    "We combine the temperature and (short) bike sales data into a single multi-series dataset. Each series gets a label in the `series` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-series dataset with both temperature and bike sales training data\n",
    "temp_hier = temp_df.copy()\n",
    "temp_hier[\"series\"] = \"temperature\"\n",
    "\n",
    "sales_hier = sales_train.copy()\n",
    "sales_hier[\"series\"] = \"bike_sales\"\n",
    "\n",
    "sales_test_hier = sales_test.copy()\n",
    "sales_test_hier[\"series\"] = \"bike_sales\"\n",
    "\n",
    "hier_df = pd.concat([temp_hier, sales_hier], ignore_index=True)\n",
    "\n",
    "print(f\"Hierarchical dataset shape: {hier_df.shape}\")\n",
    "print(\"\\nSeries breakdown:\")\n",
    "for series_name in hier_df[\"series\"].unique():\n",
    "    series_data = hier_df[hier_df[\"series\"] == series_name]\n",
    "    print(\n",
    "        f\"  {series_name}: {len(series_data)} days \"\n",
    "        f\"({series_data['ds'].min().date()} to {series_data['ds'].max().date()})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the combined dataset\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "for series_name, color, marker in [\n",
    "    (\"temperature\", \"C0\", \".\"),\n",
    "    (\"bike_sales\", \"C1\", \"o\"),\n",
    "]:\n",
    "    series_data = hier_df[hier_df[\"series\"] == series_name]\n",
    "    ax.scatter(\n",
    "        series_data[\"ds\"],\n",
    "        series_data[\"y\"],\n",
    "        c=color,\n",
    "        s=3 if marker == \".\" else 10,\n",
    "        alpha=0.5,\n",
    "        label=series_name,\n",
    "    )\n",
    "\n",
    "ax.axvline(\n",
    "    train_test_date,\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Sales train/test split\",\n",
    ")\n",
    "ax.set_title(\"Combined Dataset: Temperature + Bike Sales (Training Only)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Value (different scales)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Temperature (°F) and bike sales (counts) have very different scales.\")\n",
    "print(\"scale_mode='individual' ensures each series is scaled independently.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Hierarchical Transfer Learning Model\n",
    "\n",
    "The model uses `pool_type=\"partial\"` on the yearly seasonality with `tune_method=\"prior_from_idata\"`. This means:\n",
    "1. The **shared** yearly seasonality hyperprior is set from the temperature model's posterior via `prior_from_idata`\n",
    "2. Each series (temperature and bike sales) gets its own yearly seasonality coefficients, drawn from a hierarchical distribution centered on the shared prior\n",
    "3. Other components (trend, weekly seasonality) use the default priors, but are also partially pooled\n",
    "\n",
    "The key insight: the shared yearly seasonality hyperprior is **not learned from scratch** — it's **warm-started** with the temperature model's posterior. Partial pooling then allows each series to deviate from this informed shared pattern proportionally to the data it brings to the table.\n",
    "\n",
    "Since the temperature series has ~3 years of data, it provides a strong signal about yearly seasonality. The bike sales series (only ~3 months) cannot strongly influence the shared pattern, but it can **deviate** from it where its own data supports a different shape. This is exactly the \"borrowing strength\" that hierarchical modeling excels at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical model: partial pooling with transfer learning\n",
    "hier_model = (\n",
    "    FlatTrend(\n",
    "        pool_type=\"individual\"\n",
    "    )  # Separate flat trend for each series (no pooling)\n",
    "    + FourierSeasonality(\n",
    "        period=365.25,\n",
    "        series_order=6,\n",
    "        pool_type=\"partial\",\n",
    "        tune_method=\"prior_from_idata\",  # Shared prior from temperature posterior\n",
    "        shrinkage_strength=10,\n",
    "    )\n",
    "    + FourierSeasonality(\n",
    "        period=91.31, series_order=4, pool_type=\"partial\", shrinkage_strength=10\n",
    "    )\n",
    "    + FourierSeasonality(\n",
    "        period=30.44, series_order=3, pool_type=\"partial\", shrinkage_strength=10\n",
    "    )\n",
    "    + FourierSeasonality(\n",
    "        period=7, series_order=3, pool_type=\"partial\", shrinkage_strength=10\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Fitting hierarchical model with partial pooling + transfer learning...\")\n",
    "print(\"  - FlatTrend: individual pooling (separate flat trend for each series)\")\n",
    "print(\n",
    "    \"  - Yearly seasonality: partial pooling + prior_from_idata (shared prior from temperature)\"\n",
    ")\n",
    "print(\"  - Weekly seasonality: partial pooling (learned jointly from both series)\")\n",
    "\n",
    "hier_model.fit(\n",
    "    hier_df,\n",
    "    method=\"mapx\",\n",
    "    idata=temp_model.trace,\n",
    "    t_scale_params=temp_model.t_scale_params,\n",
    "    scaler=\"minmax\",\n",
    "    scale_mode=\"individual\",\n",
    "    sigma_pool_type=\"individual\",\n",
    ")\n",
    "\n",
    "print(f\"Group mapping: {hier_model.groups_}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with horizon covering the test period\n",
    "hier_pred = hier_model.predict(horizon=0, freq=\"D\")\n",
    "\n",
    "# Find the group code for bike_sales\n",
    "sales_group = [k for k, v in hier_model.groups_.items() if v == \"bike_sales\"][0]\n",
    "temp_group = [k for k, v in hier_model.groups_.items() if v == \"temperature\"][0]\n",
    "\n",
    "print(f\"bike_sales group code: {sales_group}\")\n",
    "print(f\"temperature group code: {temp_group}\")\n",
    "print(f\"\\nPrediction columns: {[c for c in hier_pred.columns if 'yhat' in c]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hierarchical model predictions for bike sales\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Training data\n",
    "ax.plot(\n",
    "    sales_train[\"ds\"],\n",
    "    sales_train[\"y\"],\n",
    "    \"C0o\",\n",
    "    markersize=3,\n",
    "    alpha=0.5,\n",
    "    label=\"Training data\",\n",
    ")\n",
    "\n",
    "# Test data (ground truth)\n",
    "ax.plot(\n",
    "    sales_test[\"ds\"],\n",
    "    sales_test[\"y\"],\n",
    "    \"C1o\",\n",
    "    markersize=3,\n",
    "    alpha=0.5,\n",
    "    label=\"Test data (actual)\",\n",
    ")\n",
    "\n",
    "# Hierarchical prediction (for bike_sales series)\n",
    "ax.plot(\n",
    "    hier_pred[\"ds\"],\n",
    "    hier_pred[f\"yhat_{sales_group}\"],\n",
    "    \"C3-\",\n",
    "    linewidth=1.5,\n",
    "    alpha=0.8,\n",
    "    label=\"Hierarchical + Transfer prediction\",\n",
    ")\n",
    "\n",
    "ax.axvline(\n",
    "    train_test_date, color=\"gray\", linestyle=\"--\", linewidth=2, label=\"Train/Test Split\"\n",
    ")\n",
    "ax.set_title(\"Hierarchical Model: Partial Pooling + Transfer Learning (Bike Sales)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Sales\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also visualize the temperature fit from the hierarchical model\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "ax.plot(\n",
    "    temp_df[\"ds\"],\n",
    "    temp_df[\"y\"],\n",
    "    \"C0.\",\n",
    "    markersize=1,\n",
    "    alpha=0.5,\n",
    "    label=\"Temperature data\",\n",
    ")\n",
    "ax.plot(\n",
    "    hier_pred[\"ds\"],\n",
    "    hier_pred[f\"yhat_{temp_group}\"],\n",
    "    \"r-\",\n",
    "    linewidth=1.5,\n",
    "    label=\"Hierarchical model fit\",\n",
    ")\n",
    "\n",
    "ax.set_title(\"Hierarchical Model: Temperature Series Fit\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Temperature (°F)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component decomposition for both series\n",
    "print(\"=== Bike Sales Components ===\")\n",
    "hier_model.plot(hier_pred.iloc[:-1], series=\"bike_sales\", y_true=sales_test_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Temperature Components ===\")\n",
    "hier_model.plot(hier_pred, series=\"temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Full Comparison: All Approaches\n",
    "\n",
    "Let's compare all the approaches we've seen across this notebook and Chapter 07:\n",
    "1. **Parametric transfer** — `tune_method=\"parametric\"` (Chapter 07): transfers mean/std only\n",
    "2. **`prior_from_idata` transfer** — `tune_method=\"prior_from_idata\"`: transfers full posterior\n",
    "3. **Hierarchical + transfer** — partial pooling with `prior_from_idata`: jointly models both series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare hierarchical prediction for bike sales only\n",
    "hier_sales_pred = hier_pred[[\"ds\", f\"yhat_{sales_group}\"]].copy()\n",
    "hier_sales_pred.columns = [\"ds\", \"yhat_0\"]\n",
    "\n",
    "# Calculate metrics for all approaches\n",
    "param_metrics = metrics(sales_test, transfer_param_pred, pool_type=\"complete\")\n",
    "pfi_metrics = metrics(sales_test, transfer_pfi_pred, pool_type=\"complete\")\n",
    "hier_metrics = metrics(sales_test, hier_sales_pred, pool_type=\"complete\")\n",
    "\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Parametric Transfer\": param_metrics.iloc[0],\n",
    "        \"prior_from_idata Transfer\": pfi_metrics.iloc[0],\n",
    "        \"Hierarchical + Transfer\": hier_metrics.iloc[0],\n",
    "    },\n",
    ").T\n",
    "\n",
    "print(\"Test Set Metrics Comparison\")\n",
    "print(\"=\" * 60)\n",
    "display(comparison_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay all approaches\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Data\n",
    "ax.plot(\n",
    "    sales_train[\"ds\"],\n",
    "    sales_train[\"y\"],\n",
    "    \"C0o\",\n",
    "    markersize=3,\n",
    "    alpha=0.3,\n",
    "    label=\"Training data\",\n",
    ")\n",
    "ax.plot(\n",
    "    sales_test[\"ds\"],\n",
    "    sales_test[\"y\"],\n",
    "    \"C1o\",\n",
    "    markersize=3,\n",
    "    alpha=0.3,\n",
    "    label=\"Test data (actual)\",\n",
    ")\n",
    "\n",
    "# Predictions from each approach\n",
    "ax.plot(\n",
    "    transfer_param_pred[\"ds\"],\n",
    "    transfer_param_pred[\"yhat_0\"],\n",
    "    \"C2-\",\n",
    "    linewidth=1.5,\n",
    "    alpha=0.7,\n",
    "    label=\"Parametric\",\n",
    ")\n",
    "ax.plot(\n",
    "    transfer_pfi_pred[\"ds\"],\n",
    "    transfer_pfi_pred[\"yhat_0\"],\n",
    "    \"C4-\",\n",
    "    linewidth=1.5,\n",
    "    alpha=0.7,\n",
    "    label=\"prior_from_idata\",\n",
    ")\n",
    "ax.plot(\n",
    "    hier_sales_pred[\"ds\"],\n",
    "    hier_sales_pred[\"yhat_0\"],\n",
    "    \"C3-\",\n",
    "    linewidth=1.5,\n",
    "    alpha=0.7,\n",
    "    label=\"Hierarchical + Transfer\",\n",
    ")\n",
    "\n",
    "ax.axvline(\n",
    "    train_test_date, color=\"gray\", linestyle=\"--\", linewidth=2, label=\"Train/Test Split\"\n",
    ")\n",
    "ax.set_title(\"All Transfer Learning Approaches Compared\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Sales\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### `parametric` vs `prior_from_idata`\n",
    "\n",
    "| Feature | `parametric` | `prior_from_idata` |\n",
    "|---------|-------------|-------------------|\n",
    "| What it captures | Marginal mean and std per parameter | Full joint posterior (covariance) |\n",
    "| Parameter correlations | Discarded | Preserved via multivariate Normal |\n",
    "| Implementation | `pm.Normal(mean, std)` per parameter | `pmx.utils.prior.prior_from_idata()` |\n",
    "| Computational cost | Slightly lower | Slightly higher (covariance estimation) |\n",
    "| When to prefer | Quick experiments, weakly correlated params | Production, correlated Fourier terms |\n",
    "\n",
    "### Hierarchical + Transfer Learning\n",
    "\n",
    "Combining partial pooling with `prior_from_idata` is vangja's most powerful forecasting pattern for short time series:\n",
    "1. **Transfer learning** brings in external seasonal knowledge from a long related series\n",
    "2. **Partial pooling** allows the short target series to deviate from the transferred prior where its own data supports it\n",
    "3. **Joint modeling** means both series contribute to the shared seasonal hyperprior, with the long series naturally dominating due to having more data\n",
    "\n",
    "This approach is especially valuable when you have:\n",
    "- A long \"reference\" series with clear seasonality (e.g., weather, economic indicators)\n",
    "- One or more short \"target\" series that share seasonal drivers but lack enough history to learn them independently\n",
    "- A desire to quantify uncertainty about the seasonal transfer\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [pymc-extras prior_from_idata API docs](https://www.pymc.io/projects/extras/en/stable/generated/pymc_extras.utils.prior.prior_from_idata.html) — Full documentation for the `prior_from_idata` function\n",
    "- [Updating Priors (PyMC example)](https://www.pymc.io/projects/examples/en/latest/howto/updating_priors.html) — The original inspiration for iterative prior updating in PyMC\n",
    "- [Modeling Short Time Series with Prior Knowledge](https://minimizeregret.com/short-time-series-prior-knowledge) — Tim Radtke's original blog post\n",
    "\n",
    "### What's Next\n",
    "\n",
    "**Chapter 09** documents several **advanced transfer learning options** — bidirectional changepoints, regularization potentials, phase alignment, and custom summary statistics — that provide fine-grained control over the transfer process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vangja20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
