{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeca849d",
   "metadata": {},
   "source": [
    "# Chapter 07: Transfer Learning from Long Time Series\n",
    "\n",
    "Forecasting short time series is notoriously difficult. With only a few weeks or months of data, traditional methods struggle to capture long-term patterns like yearly seasonality. Yet in many business contexts, we *know* such patterns exist — we just haven't observed them yet.\n",
    "\n",
    "This chapter demonstrates how to use **transfer learning** to overcome this limitation. The key insight: if we have a long time series that shares seasonal patterns with our short target series, we can:\n",
    "\n",
    "1. Fit a model to the long series to learn its seasonal structure\n",
    "2. Transfer the learned posteriors as priors to the short series model\n",
    "\n",
    "This approach was inspired by:\n",
    "- [Modeling Short Time Series with Prior Knowledge](https://minimizeregret.com/short-time-series-prior-knowledge) by Tim Radtke\n",
    "- [Modeling Short Time Series with Prior Knowledge in PyMC](https://juanitorduz.github.io/short_time_series_pymc/) by Juan Orduz\n",
    "\n",
    "Vangja was partially inspired by these two blog posts and implements parametric transfer learning as a core feature.\n",
    "\n",
    "## The Scenario\n",
    "\n",
    "We have daily bike sales data from a Citi Bike station in NYC, but only **~3 months** of history (July–October 2013). We need to forecast sales for the next year, including through winter and back to summer. \n",
    "\n",
    "The challenge: with only summer/fall data, how can we capture the yearly seasonality pattern where demand drops significantly in winter?\n",
    "\n",
    "**The solution**: New York City temperature has a strong yearly pattern that correlates with bike demand. Even though we can't use temperature directly as a predictor (we'd need future forecasts), we *can* learn the shape of yearly seasonality from historical temperature data and transfer it to our sales model.\n",
    "\n",
    "## Differences from the Original Blog Posts\n",
    "\n",
    "The original implementations use:\n",
    "- A negative binomial likelihood with autoregressive (AR) components on the latent mean\n",
    "- A damped dynamic model: $\\mu_t = (1-\\delta-\\eta)\\lambda_t + \\delta\\mu_{t-1} + \\eta y_{t-1}$\n",
    "\n",
    "Vangja uses a simpler Prophet-like approach:\n",
    "- A piecewise `LinearTrend` component instead of AR dynamics\n",
    "- Gaussian likelihood\n",
    "- Transfer learning via the `tune_method=\"parametric\"` parameter on `FourierSeasonality`\n",
    "\n",
    "Despite these differences, the core insight — transferring learned seasonality via Bayesian priors — works remarkably well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd97d4",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91000f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b7d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vangja import LinearTrend, FourierSeasonality, FlatTrend\n",
    "from vangja.datasets import load_citi_bike_sales, load_nyc_temperature\n",
    "from vangja.utils import metrics\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45bd19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load and Explore the Data\n",
    "\n",
    "We load two datasets:\n",
    "1. **Temperature data**: ~5 years of daily max temperatures for NYC (our \"long\" series)\n",
    "2. **Bike sales data**: ~16 months of daily bike rides from Citi Bike station 360 (we'll use only ~3 months for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "temp_df = load_nyc_temperature()\n",
    "sales_df = load_citi_bike_sales()\n",
    "\n",
    "# The original Tim Radtke blog post used the following date ranges\n",
    "temp_df = temp_df[\n",
    "    (temp_df[\"ds\"] >= sales_df[\"ds\"].min()) & (temp_df[\"ds\"] <= sales_df[\"ds\"].max())\n",
    "]\n",
    "\n",
    "print(\"Temperature data:\")\n",
    "print(f\"  Shape: {temp_df.shape}\")\n",
    "print(f\"  Date range: {temp_df['ds'].min().date()} to {temp_df['ds'].max().date()}\")\n",
    "print(f\"  Days: {(temp_df['ds'].max() - temp_df['ds'].min()).days}\")\n",
    "\n",
    "print(\"\\nSales data:\")\n",
    "print(f\"  Shape: {sales_df.shape}\")\n",
    "print(f\"  Date range: {sales_df['ds'].min().date()} to {sales_df['ds'].max().date()}\")\n",
    "print(f\"  Days: {(sales_df['ds'].max() - sales_df['ds'].min()).days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00da1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both time series\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n",
    "\n",
    "# Temperature\n",
    "axes[0].plot(temp_df[\"ds\"], temp_df[\"y\"], \"C0-\", linewidth=0.5, alpha=0.7)\n",
    "axes[0].set_title(\"NYC Daily Max Temperature (Long Series - 3 years)\")\n",
    "axes[0].set_ylabel(\"Temperature (°F)\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sales\n",
    "axes[1].plot(sales_df[\"ds\"], sales_df[\"y\"], \"C1-\", linewidth=0.5, alpha=0.7)\n",
    "axes[1].set_title(\"Citi Bike Station 360 Daily Sales (Full Series)\")\n",
    "axes[1].set_ylabel(\"Number of Sales\")\n",
    "axes[1].set_xlabel(\"Date\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e03512",
   "metadata": {},
   "source": [
    "### Create the Short Training Set\n",
    "\n",
    "Following the original blog post, we use only the first ~3 months of sales data for training (up to October 15, 2013). The rest is held out as our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train/test split date\n",
    "train_test_date = pd.to_datetime(\"2013-10-15\")\n",
    "\n",
    "# Split the sales data\n",
    "sales_train = sales_df[sales_df[\"ds\"] < train_test_date].copy()\n",
    "sales_test = sales_df[sales_df[\"ds\"] >= train_test_date].copy()\n",
    "\n",
    "print(\n",
    "    f\"Training period: {sales_train['ds'].min().date()} to {sales_train['ds'].max().date()}\"\n",
    ")\n",
    "print(f\"Training samples: {len(sales_train)} days\")\n",
    "print(\n",
    "    f\"\\nTest period: {sales_test['ds'].min().date()} to {sales_test['ds'].max().date()}\"\n",
    ")\n",
    "print(f\"Test samples: {len(sales_test)} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ab93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the train/test split\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "ax.plot(\n",
    "    sales_train[\"ds\"],\n",
    "    sales_train[\"y\"],\n",
    "    \"C0o-\",\n",
    "    markersize=2,\n",
    "    linewidth=0.5,\n",
    "    alpha=0.7,\n",
    "    label=\"Training\",\n",
    ")\n",
    "ax.plot(\n",
    "    sales_test[\"ds\"],\n",
    "    sales_test[\"y\"],\n",
    "    \"C1o-\",\n",
    "    markersize=2,\n",
    "    linewidth=0.5,\n",
    "    alpha=0.7,\n",
    "    label=\"Test (holdout)\",\n",
    ")\n",
    "ax.axvline(\n",
    "    train_test_date, color=\"gray\", linestyle=\"--\", linewidth=2, label=\"Train/Test Split\"\n",
    ")\n",
    "ax.set_title(\"Sales Data: Train/Test Split\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Sales\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe challenge: We need to forecast through winter with only summer/fall data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d9a6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Baseline: Prophet-like Model Without Transfer Learning\n",
    "\n",
    "First, let's see what happens when we fit a standard Prophet-like model to the short training data — with yearly seasonality but no prior knowledge about its shape.\n",
    "\n",
    "This is what most practitioners would try first, and it's exactly what fails in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ed8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model: Prophet-like with uninformed yearly seasonality\n",
    "baseline_model = (\n",
    "    LinearTrend(n_changepoints=5)  # Linear trend with 5 changepoints seems to work best\n",
    "    + FourierSeasonality(period=365.25, series_order=6)  # Yearly\n",
    "    + FourierSeasonality(period=91.31, series_order=4)  # Quarterly\n",
    "    + FourierSeasonality(period=30.44, series_order=3)  # Monthly\n",
    "    + FourierSeasonality(period=7, series_order=2)  # Weekly\n",
    ")\n",
    "\n",
    "print(\"Fitting baseline model (no transfer learning)...\")\n",
    "baseline_model.fit(sales_train, method=\"mapx\", scaler=\"minmax\")\n",
    "\n",
    "# Predict over the full date range (train + test)\n",
    "baseline_pred = baseline_model.predict(horizon=len(sales_test), freq=\"D\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline predictions vs actual data\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Training data\n",
    "ax.plot(\n",
    "    sales_train[\"ds\"],\n",
    "    sales_train[\"y\"],\n",
    "    \"C0o\",\n",
    "    markersize=3,\n",
    "    alpha=0.5,\n",
    "    label=\"Training data\",\n",
    ")\n",
    "\n",
    "# Test data (ground truth)\n",
    "ax.plot(\n",
    "    sales_test[\"ds\"],\n",
    "    sales_test[\"y\"],\n",
    "    \"C1o\",\n",
    "    markersize=3,\n",
    "    alpha=0.5,\n",
    "    label=\"Test data (actual)\",\n",
    ")\n",
    "\n",
    "# Baseline prediction\n",
    "ax.plot(\n",
    "    baseline_pred[\"ds\"],\n",
    "    baseline_pred[\"yhat_0\"],\n",
    "    \"C3-\",\n",
    "    linewidth=1.5,\n",
    "    alpha=0.8,\n",
    "    label=\"Baseline prediction\",\n",
    ")\n",
    "\n",
    "ax.axvline(\n",
    "    train_test_date, color=\"gray\", linestyle=\"--\", linewidth=2, label=\"Train/Test Split\"\n",
    ")\n",
    "ax.set_title(\"Baseline Model: Prophet-like without Transfer Learning\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Rides\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: The baseline model fails to capture yearly seasonality!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a6f08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Learn Seasonality from Temperature Data\n",
    "\n",
    "Now we fit a model to the long temperature time series. The goal is to learn the shape of yearly seasonality — specifically, the posterior distribution of the Fourier coefficients.\n",
    "\n",
    "Since temperature is much simpler than sales (just yearly seasonality, no weekly pattern, minimal trend), we use a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature model: Simple trend + yearly seasonality\n",
    "# We use a very simple flat trend since we do not expect temperature to have growth\n",
    "temp_model = FlatTrend() + FourierSeasonality(period=365.25, series_order=6)\n",
    "\n",
    "print(\"Fitting temperature model to learn yearly seasonality...\")\n",
    "temp_model.fit(temp_df, method=\"nuts\", scaler=\"minmax\")\n",
    "\n",
    "# Get predictions to visualize the fit\n",
    "temp_pred = temp_model.predict(horizon=0, freq=\"D\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature model fit\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "ax.plot(temp_df[\"ds\"], temp_df[\"y\"], \"C0.\", markersize=1, label=\"Temperature data\")\n",
    "ax.plot(temp_pred[\"ds\"], temp_pred[\"yhat_0\"], \"r-\", linewidth=1.5, label=\"Model fit\")\n",
    "\n",
    "ax.set_title(\"Temperature Model: Learning Yearly Seasonality\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Temperature (°F)\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the component decomposition for the temperature model\n",
    "temp_model.plot(temp_pred, y_true=temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac6e5b",
   "metadata": {},
   "source": [
    "### The Learned Fourier Coefficients\n",
    "\n",
    "The temperature model has learned 12 Fourier coefficients (6 sine + 6 cosine terms) that describe the yearly seasonal pattern. These coefficients, stored in `trace`, will be transferred as priors to the sales model.\n",
    "\n",
    "Using the `tune_method=\"parametric\"` option, vangja automatically extracts the posterior mean and standard deviation of these coefficients and uses them as the prior for the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the learned Fourier coefficients from the temperature model\n",
    "# These are stored in the trace attribute after fitting\n",
    "beta_key = \"fs_0 - beta(p=365.25,n=6)\"\n",
    "beta_posterior = temp_model.trace[\"posterior\"][beta_key]\n",
    "\n",
    "# Calculate mean and std (what parametric tuning will use)\n",
    "beta_mean = beta_posterior.mean(dim=[\"chain\", \"draw\"]).values\n",
    "beta_std = beta_posterior.std(dim=[\"chain\", \"draw\"]).values\n",
    "\n",
    "print(\"Learned Fourier coefficients from temperature model:\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(beta_mean)):\n",
    "    term_type = \"sin\" if i % 2 == 0 else \"cos\"\n",
    "    order = (i // 2) + 1\n",
    "    print(\n",
    "        f\"  {term_type}(order={order}): mean = {beta_mean[i]:7.3f}, std = {beta_std[i]:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nThese posteriors will become the priors for the sales model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7255a319",
   "metadata": {},
   "source": [
    "As in any transfer learning scenario, we also need to be careful to extract the scaling parameters for the timestamp column in `temp_model.data`. Let's take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae44565",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model.t_scale_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02741d74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Transfer Learning to the Sales Model\n",
    "\n",
    "Now we create a new model for sales that:\n",
    "1. Has its own trend and weekly seasonality (learned from sales data)\n",
    "2. **Transfers** the yearly seasonality from the temperature model via `tune_method=\"parametric\"`\n",
    "\n",
    "The `tune_method=\"parametric\"` setting tells the `FourierSeasonality` component to:\n",
    "- Extract the posterior mean and std of the Fourier coefficients from `idata`\n",
    "- Use these as the prior mean and std for the new model's coefficients\n",
    "\n",
    "This gives the sales model a strong, informed prior about what yearly seasonality looks like — even though it has never seen a full year of sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning model: Use learned yearly seasonality\n",
    "transfer_model = (\n",
    "    FlatTrend()  # No trend, since we have very little data and don't want to overfit a trend\n",
    "    + FourierSeasonality(\n",
    "        period=365.25,\n",
    "        series_order=6,\n",
    "        tune_method=\"parametric\",  # KEY: Transfer from temperature model\n",
    "    )\n",
    "    + FourierSeasonality(period=91.31, series_order=4)  # Quarterly (learned from sales)\n",
    "    + FourierSeasonality(period=30.44, series_order=3)  # Monthly (learned from sales)\n",
    "    + FourierSeasonality(period=7, series_order=3)  # Weekly (learned from sales)\n",
    ")\n",
    "\n",
    "print(\"Fitting transfer learning model...\")\n",
    "print(\"  - Flat trend: no trend learned from sales data\")\n",
    "print(\"  - Weekly seasonality: learned from sales data\")\n",
    "print(\"  - Monthly seasonality: learned from sales data\")\n",
    "print(\"  - Quarterly seasonality: learned from sales data\")\n",
    "print(\"  - Yearly seasonality: TRANSFERRED from temperature model\")\n",
    "\n",
    "# Pass the temperature model's trace and scaling parameters to transfer the learned seasonality\n",
    "transfer_model.fit(\n",
    "    sales_train.iloc[2:],\n",
    "    method=\"mapx\",\n",
    "    idata=temp_model.trace,\n",
    "    t_scale_params=temp_model.t_scale_params,\n",
    "    scaler=\"minmax\",\n",
    ")\n",
    "\n",
    "# Predict over the full date range\n",
    "transfer_pred = transfer_model.predict(horizon=len(sales_test), freq=\"D\")\n",
    "\n",
    "# Keep only the predictions for the train + test period (since predict returns the full range)\n",
    "transfer_pred = transfer_pred[\n",
    "    (transfer_pred[\"ds\"] >= sales_train[\"ds\"].min())\n",
    "    & (transfer_pred[\"ds\"] <= sales_test[\"ds\"].max())\n",
    "]\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot transfer learning predictions vs actual data\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Training data\n",
    "ax.plot(\n",
    "    sales_train[\"ds\"],\n",
    "    sales_train[\"y\"],\n",
    "    \"C0o\",\n",
    "    markersize=3,\n",
    "    alpha=0.5,\n",
    "    label=\"Training data\",\n",
    ")\n",
    "\n",
    "# Test data (ground truth)\n",
    "ax.plot(\n",
    "    sales_test[\"ds\"],\n",
    "    sales_test[\"y\"],\n",
    "    \"C1o\",\n",
    "    markersize=3,\n",
    "    alpha=0.5,\n",
    "    label=\"Test data (actual)\",\n",
    ")\n",
    "\n",
    "# Transfer learning prediction\n",
    "ax.plot(\n",
    "    transfer_pred[\"ds\"],\n",
    "    transfer_pred[\"yhat_0\"],\n",
    "    \"C2-\",\n",
    "    linewidth=1.5,\n",
    "    alpha=0.8,\n",
    "    label=\"Transfer learning prediction\",\n",
    ")\n",
    "\n",
    "ax.axvline(\n",
    "    train_test_date, color=\"gray\", linestyle=\"--\", linewidth=2, label=\"Train/Test Split\"\n",
    ")\n",
    "ax.set_title(\"Transfer Learning Model: Yearly Seasonality from Temperature\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Sales\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: The transfer learning model captures the winter drop!\")\n",
    "print(\"It correctly predicts lower demand in winter and recovery in spring/summer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot component decomposition for the transfer learning model\n",
    "transfer_model.plot(transfer_pred, y_true=sales_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db458d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparison: Baseline vs Transfer Learning\n",
    "\n",
    "Let's compare both models side-by-side to see the dramatic improvement from transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "for ax, (model_name, pred, color) in zip(\n",
    "    axes,\n",
    "    [\n",
    "        (\"Baseline (no transfer)\", baseline_pred, \"C3\"),\n",
    "        (\"Transfer Learning\", transfer_pred, \"C2\"),\n",
    "    ],\n",
    "):\n",
    "    # Training data\n",
    "    ax.plot(\n",
    "        sales_train[\"ds\"],\n",
    "        sales_train[\"y\"],\n",
    "        \"C0o\",\n",
    "        markersize=3,\n",
    "        alpha=0.5,\n",
    "        label=\"Training data\",\n",
    "    )\n",
    "\n",
    "    # Test data (ground truth)\n",
    "    ax.plot(\n",
    "        sales_test[\"ds\"],\n",
    "        sales_test[\"y\"],\n",
    "        \"C1o\",\n",
    "        markersize=3,\n",
    "        alpha=0.5,\n",
    "        label=\"Test data (actual)\",\n",
    "    )\n",
    "\n",
    "    # Model prediction\n",
    "    ax.plot(\n",
    "        pred[\"ds\"],\n",
    "        pred[\"yhat_0\"],\n",
    "        f\"{color}-\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.8,\n",
    "        label=f\"{model_name} prediction\",\n",
    "    )\n",
    "\n",
    "    ax.axvline(train_test_date, color=\"gray\", linestyle=\"--\", linewidth=2)\n",
    "    ax.set_title(f\"{model_name}\")\n",
    "    ax.set_ylabel(\"Number of Sales\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel(\"Date\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Training data\n",
    "ax.plot(\n",
    "    sales_train[\"ds\"],\n",
    "    sales_train[\"y\"],\n",
    "    \"C0o\",\n",
    "    markersize=3,\n",
    "    alpha=0.4,\n",
    "    label=\"Training data\",\n",
    ")\n",
    "\n",
    "# Test data (ground truth)\n",
    "ax.plot(\n",
    "    sales_test[\"ds\"],\n",
    "    sales_test[\"y\"],\n",
    "    \"C1o\",\n",
    "    markersize=3,\n",
    "    alpha=0.4,\n",
    "    label=\"Test data (actual)\",\n",
    ")\n",
    "\n",
    "# Both predictions\n",
    "ax.plot(\n",
    "    baseline_pred[\"ds\"],\n",
    "    baseline_pred[\"yhat_0\"],\n",
    "    \"C3-\",\n",
    "    linewidth=1.5,\n",
    "    alpha=0.8,\n",
    "    label=\"Baseline prediction\",\n",
    ")\n",
    "ax.plot(\n",
    "    transfer_pred[\"ds\"],\n",
    "    transfer_pred[\"yhat_0\"],\n",
    "    \"C2-\",\n",
    "    linewidth=1.5,\n",
    "    alpha=0.8,\n",
    "    label=\"Transfer learning prediction\",\n",
    ")\n",
    "\n",
    "ax.axvline(\n",
    "    train_test_date, color=\"gray\", linestyle=\"--\", linewidth=2, label=\"Train/Test Split\"\n",
    ")\n",
    "ax.set_title(\"Comparison: Baseline vs Transfer Learning\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Sales\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e7a1c",
   "metadata": {},
   "source": [
    "### Quantitative Comparison\n",
    "\n",
    "Let's compute error metrics on the test set to quantify the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b925cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "baseline_metrics = metrics(sales_test, baseline_pred, pool_type=\"complete\")\n",
    "transfer_metrics = metrics(sales_test, transfer_pred, pool_type=\"complete\")\n",
    "\n",
    "# Also compute a simple mean baseline\n",
    "mean_pred_value = sales_train[\"y\"].mean()\n",
    "mean_metrics = pd.DataFrame(\n",
    "    {\n",
    "        \"mse\": {\"series\" : np.mean((sales_test[\"y\"].values - mean_pred_value) ** 2)},\n",
    "        \"rmse\": {\"series\" : np.sqrt(np.mean((sales_test[\"y\"].values - mean_pred_value) ** 2))},\n",
    "        \"mae\": {\"series\" : np.mean(np.abs(sales_test[\"y\"].values - mean_pred_value))},\n",
    "        \"mape\": {\"series\" : np.mean(\n",
    "            np.abs((sales_test[\"y\"].values - mean_pred_value) / sales_test[\"y\"].values)\n",
    "        ) * 100},\n",
    "    },\n",
    "    index=[\"series\"]\n",
    ")\n",
    "\n",
    "# Display comparison table\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Mean Baseline\": mean_metrics.iloc[0],\n",
    "        \"Prophet-like (no transfer)\": baseline_metrics.iloc[0],\n",
    "        \"Transfer Learning\": transfer_metrics.iloc[0],\n",
    "    },\n",
    ").T\n",
    "\n",
    "print(\"Test Set Metrics Comparison\")\n",
    "print(\"=\" * 60)\n",
    "display(comparison_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bc76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement percentages\n",
    "rmse_improvement = (1 - transfer_metrics[\"rmse\"] / baseline_metrics[\"rmse\"]) * 100\n",
    "mae_improvement = (1 - transfer_metrics[\"mae\"] / baseline_metrics[\"mae\"]) * 100\n",
    "\n",
    "print(\"\\nImprovement from Transfer Learning:\")\n",
    "print(f\"  RMSE reduction: {rmse_improvement.values[0]:.1f}%\")\n",
    "print(f\"  MAE reduction:  {mae_improvement.values[0]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98792921",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How It Works: The Parametric Transfer\n",
    "\n",
    "The `tune_method=\"parametric\"` option in vangja implements a simple but powerful form of Bayesian transfer learning:\n",
    "\n",
    "### Step 1: Fit the Source Model\n",
    "\n",
    "You fit the temperature model to the long time series using MCMC sampling or Variational Inference (Maximum A Posteriori estimates do not produce posterior uncertainty). For example:\n",
    "\n",
    "```python\n",
    "temp_model.fit(temp_df, method=\"nuts\")\n",
    "```\n",
    "The temperature model learns Fourier coefficients $\\beta_1, \\ldots, \\beta_{12}$ from ~5 years of data. The posterior is stored in `temp_model.idata_`.\n",
    "\n",
    "### Step 2: Extract Posterior Statistics\n",
    "When fitting the sales model with `idata=temp_model.trace`, vangja extracts:\n",
    "- $\\mu_i = \\mathbb{E}[\\beta_i | \\text{data}]$ — posterior mean\n",
    "- $\\sigma_i = \\text{Std}[\\beta_i | \\text{data}]$ — posterior standard deviation\n",
    "\n",
    "You can manually extract the posterior samples and compute different statistics if you want. Vangja allows you to pass these directly as `override_...` parameters in every component. Read the API docs for details.\n",
    "\n",
    "### Step 3: Set Informed Priors\n",
    "The sales model's Fourier coefficients receive Normal priors:\n",
    "\n",
    "$$\\beta^{\\text{sales}}_i \\sim \\text{Normal}(\\mu_i, \\sigma_i)$$\n",
    "\n",
    "This is much more informative than the default $\\beta \\sim \\text{Normal}(0, 10)$. The sales model's optimization process now starts with a strong belief about the seasonal shape, learned from temperature.\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "Temperature and bike sales share a causal driver: **weather/seasons**. When temperature drops in winter:\n",
    "- Fewer people want to bike in the cold\n",
    "- The yearly seasonal pattern in temperature *correlates* with bike demand\n",
    "\n",
    "By learning the shape of yearly seasonality from temperature (where we have years of data), we can transfer this knowledge to sales (where we have only months)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd3bfa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Limitations and Considerations\n",
    "\n",
    "### What We Simplified\n",
    "\n",
    "The original blog posts use a more sophisticated model:\n",
    "- **Negative binomial likelihood** for count data (we use Gaussian)\n",
    "- **Autoregressive dynamics**: $\\mu_t = (1-\\delta-\\eta)\\lambda_t + \\delta\\mu_{t-1} + \\eta y_{t-1}$\n",
    "- **Day-of-week effects** as explicit dummy variables\n",
    "\n",
    "Vangja's approach is simpler:\n",
    "- We use a Gaussian likelihood (adequate for this scale of data)\n",
    "- Instead of AR dynamics, we use `LinearTrend` with changepoints\n",
    "- Weekly seasonality is learned via `FourierSeasonality(period=7)`\n",
    "- We add a monthly and quarterly Fourier seasonality to capture additional patterns\n",
    "\n",
    "Despite these simplifications, the core insight — transferring learned seasonality via Bayesian priors — works well.\n",
    "\n",
    "### When Transfer Learning Helps\n",
    "\n",
    "Transfer learning is most valuable when:\n",
    "1. **Short target series**: Not enough data to learn long-term patterns\n",
    "2. **Long source series**: Available related data with the pattern you need\n",
    "3. **Shared patterns**: The source and target genuinely share the pattern to transfer\n",
    "\n",
    "### Potential Pitfalls\n",
    "\n",
    "1. **Over-confident priors**: If the temperature seasonality doesn't match sales at all, strong priors could hurt. Consider overriding the `beta_sd` parameter for tuning or using less restrictive priors.\n",
    "\n",
    "2. **Scale mismatch**: Temperature is in °F, sales is in counts. Vangja handles this by scaling data internally, and the Fourier coefficients represent *shape*, not magnitude.\n",
    "\n",
    "3. **Phase mismatch**: If the seasonal peaks don't align (e.g., sales peak occurs 2 weeks after temperature peak), you may need the `shift_for_tune` option. Look at the API docs for details on how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd6446",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this chapter, we demonstrated how to use **transfer learning** to forecast short time series with vangja:\n",
    "\n",
    "1. **The Problem**: With only ~3 months of bike sales data, standard methods fail to capture yearly seasonality, leading to wildly optimistic winter forecasts.\n",
    "\n",
    "2. **The Solution**: Learn yearly seasonality from a related long time series (NYC temperature, ~5 years) and transfer it as informed priors to the sales model.\n",
    "\n",
    "3. **The Implementation**:\n",
    "   - Fit a model to temperature data to learn Fourier coefficients\n",
    "   - Create a sales model with `tune_method=\"parametric\"` on yearly seasonality\n",
    "   - Pass `idata=temp_model.idata_` to transfer the learned posteriors as priors\n",
    "\n",
    "4. **The Result**: The transfer learning model captures the winter demand drop and achieves significantly lower forecast errors than the baseline.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Transfer learning in vangja** uses `tune_method=\"parametric\"` to pass posterior mean/std as new priors\n",
    "- **Find a proxy**: When your target series is short, look for a longer related series that shares patterns\n",
    "- **Domain knowledge matters**: The insight that temperature correlates with bike demand made this transfer work\n",
    "- **Simple can work**: Even with Vangja's simpler Prophet-like approach (vs. the original AR model), the core transfer learning insight delivers strong results\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Modeling Short Time Series with Prior Knowledge](https://minimizeregret.com/short-time-series-prior-knowledge) — the original blog post by Tim Radtke\n",
    "- [Modeling Short Time Series with Prior Knowledge in PyMC](https://juanitorduz.github.io/short_time_series_pymc/) — Juan Orduz's PyMC implementation\n",
    "- [Rob Hyndman: Fitting Models to Short Time Series](https://robjhyndman.com/hyndsight/short-time-series/) — context on the challenges of short series forecasting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vangja20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
