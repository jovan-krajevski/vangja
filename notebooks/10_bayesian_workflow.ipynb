{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96e6cb7f",
   "metadata": {},
   "source": [
    "# Chapter 10: Bayesian Workflow — Diagnostics and Best Practices\n",
    "\n",
    "We revisit the transfer learning scenario from Chapter 07 (NYC temperature → Citi Bike sales) and augment it with:\n",
    "\n",
    "1. **Prior predictive checks** — Do the priors produce plausible data *before* fitting?\n",
    "2. **Convergence diagnostics** — R-hat, ESS, trace plots, energy plots\n",
    "3. **Posterior predictive checks** — Does the fitted model generate data that looks like the observations?\n",
    "4. **Model comparison** — WAIC / LOO-CV between baseline and transfer models\n",
    "5. **Prior sensitivity analysis** — How sensitive are forecasts to prior choices?\n",
    "6. **Full posterior summaries** — HDI, credible intervals, not just point estimates\n",
    "7. **Prior-to-posterior visualisation** — Showing how the data updates our beliefs\n",
    "\n",
    "These steps form the core of the WAMBS (When to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e843f",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca30d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "\n",
    "from vangja import LinearTrend, FourierSeasonality, FlatTrend\n",
    "from vangja.datasets import load_citi_bike_sales, load_nyc_temperature\n",
    "from vangja.utils import (\n",
    "    metrics,\n",
    "    compare_models,\n",
    "    plot_prior_predictive,\n",
    "    plot_posterior_predictive,\n",
    "    plot_prior_posterior,\n",
    "    prior_sensitivity_analysis,\n",
    ")\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b8f6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Loading\n",
    "\n",
    "We reproduce the setup from Chapter 07: a long NYC temperature series (source) and a\n",
    "short Citi Bike sales series (target with ~3 months training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df643d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "temp_df = load_nyc_temperature()\n",
    "sales_df = load_citi_bike_sales()\n",
    "\n",
    "# Match date ranges as in the original blog post\n",
    "temp_df = temp_df[\n",
    "    (temp_df[\"ds\"] >= sales_df[\"ds\"].min()) & (temp_df[\"ds\"] <= sales_df[\"ds\"].max())\n",
    "]\n",
    "\n",
    "# Train/test split for sales\n",
    "train_test_date = pd.to_datetime(\"2013-10-15\")\n",
    "sales_train = sales_df[sales_df[\"ds\"] < train_test_date].copy()\n",
    "sales_test = sales_df[sales_df[\"ds\"] >= train_test_date].copy()\n",
    "\n",
    "print(\n",
    "    f\"Temperature: {temp_df.shape[0]} days  ({temp_df['ds'].min().date()} – {temp_df['ds'].max().date()})\"\n",
    ")\n",
    "print(\n",
    "    f\"Sales train: {sales_train.shape[0]} days ({sales_train['ds'].min().date()} – {sales_train['ds'].max().date()})\"\n",
    ")\n",
    "print(\n",
    "    f\"Sales test:  {sales_test.shape[0]} days ({sales_test['ds'].min().date()} – {sales_test['ds'].max().date()})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(14, 7))\n",
    "\n",
    "axes[0].plot(temp_df[\"ds\"], temp_df[\"y\"], \"C0-\", lw=0.5, alpha=0.7)\n",
    "axes[0].set_title(\"NYC Daily Max Temperature (source series)\")\n",
    "axes[0].set_ylabel(\"Temperature (°F)\")\n",
    "\n",
    "axes[1].plot(sales_train[\"ds\"], sales_train[\"y\"], \"C0o-\", ms=2, lw=0.5, label=\"Train\")\n",
    "axes[1].plot(sales_test[\"ds\"], sales_test[\"y\"], \"C1o-\", ms=2, lw=0.5, label=\"Test\")\n",
    "axes[1].axvline(train_test_date, color=\"gray\", ls=\"--\", lw=2)\n",
    "axes[1].set_title(\"Citi Bike Sales (target series)\")\n",
    "axes[1].set_ylabel(\"Rides\")\n",
    "axes[1].legend()\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468f3ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Prior Predictive Checks\n",
    "\n",
    "**Why?** Before fitting, we should verify that our priors produce data in a\n",
    "plausible range. If the prior predictive distribution generates wildly\n",
    "unreasonable values, our priors need adjustment. This is the first step in\n",
    "the WAMBS checklist.\n",
    "\n",
    "We compare prior predictives from:\n",
    "- A **baseline** model (default uninformed priors)\n",
    "- The **temperature** model (simple priors, long data)\n",
    "\n",
    "### 2.1 Temperature Model — Prior Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f3fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the temperature model\n",
    "temp_model = FlatTrend() + FourierSeasonality(period=365.25, series_order=6)\n",
    "\n",
    "# We need to fit first to build the PyMC model graph, then sample from the prior\n",
    "# Fit with NUTS for full posterior (needed for diagnostics later)\n",
    "print(\"Fitting temperature model with NUTS...\")\n",
    "temp_model.fit(temp_df, method=\"nuts\", scaler=\"minmax\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec5cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the prior predictive\n",
    "temp_prior_pred = temp_model.sample_prior_predictive(samples=500)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "plot_prior_predictive(\n",
    "    temp_prior_pred,\n",
    "    data=temp_model.data,\n",
    "    n_samples=100,\n",
    "    ax=ax,\n",
    "    title=\"Temperature Model — Prior Predictive Check\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each thin blue line is one draw from the prior.\")\n",
    "print(\"The orange line is the observed temperature data.\")\n",
    "print(\"Prior predictive should at least *contain* the observed data range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2f723",
   "metadata": {},
   "source": [
    "### 2.2 Baseline Sales Model — Prior Predictive\n",
    "\n",
    "The baseline model uses default uninformative priors for yearly seasonality.\n",
    "With only ~3 months of data, the prior predictive may be very wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model (same as Ch07 but fit with NUTS for diagnostics)\n",
    "baseline_model = (\n",
    "    LinearTrend(n_changepoints=5)\n",
    "    + FourierSeasonality(period=365.25, series_order=6)\n",
    "    + FourierSeasonality(period=91.31, series_order=4)\n",
    "    + FourierSeasonality(period=30.44, series_order=3)\n",
    "    + FourierSeasonality(period=7, series_order=2)\n",
    ")\n",
    "\n",
    "print(\"Fitting baseline sales model with NUTS...\")\n",
    "baseline_model.fit(sales_train, method=\"nuts\", scaler=\"minmax\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bfb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prior_pred = baseline_model.sample_prior_predictive(samples=500)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "plot_prior_predictive(\n",
    "    baseline_prior_pred,\n",
    "    data=baseline_model.data,\n",
    "    n_samples=100,\n",
    "    ax=ax,\n",
    "    title=\"Baseline Sales Model — Prior Predictive Check\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"With default priors, the prior predictive is very diffuse.\")\n",
    "print(\"This is expected for weakly informative priors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc0b26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Convergence Diagnostics\n",
    "\n",
    "After fitting with MCMC, we **must** verify that the sampler has converged. Key checks:\n",
    "\n",
    "| Diagnostic | Threshold | Meaning |\n",
    "|---|---|---|\n",
    "| R-hat | < 1.01 | Chains have mixed well |\n",
    "| ESS bulk | > 400 | Enough effective samples for the bulk of the posterior |\n",
    "| ESS tail | > 400 | Enough effective samples for the tails |\n",
    "| BFMI | > 0.3 | No energy transition problems (NUTS) |\n",
    "| Divergences | 0 | No pathological geometry |\n",
    "\n",
    "### 3.1 Temperature Model Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ec095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence summary for the temperature model\n",
    "temp_summary = temp_model.convergence_summary()\n",
    "display(temp_summary)\n",
    "\n",
    "# Check thresholds\n",
    "rhat_ok = (temp_summary[\"r_hat\"] < 1.01).all()\n",
    "ess_ok = (temp_summary[\"ess_bulk\"] > 400).all() and (\n",
    "    temp_summary[\"ess_tail\"] > 400\n",
    ").all()\n",
    "print(f\"\\nR-hat < 1.01 for all parameters: {rhat_ok}\")\n",
    "print(f\"ESS > 400 for all parameters:     {ess_ok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c411c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots — visual inspection of mixing\n",
    "temp_model.plot_trace()\n",
    "plt.suptitle(\"Temperature Model — Trace Plots\", y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy plot (BFMI diagnostic for NUTS)\n",
    "temp_model.plot_energy()\n",
    "plt.suptitle(\"Temperature Model — Energy Diagnostic\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The marginal energy and energy transition distributions should overlap well.\")\n",
    "print(\"Large discrepancy indicates the sampler struggles to explore the posterior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13cb7e",
   "metadata": {},
   "source": [
    "### 3.2 Baseline Sales Model Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6783868",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_summary = baseline_model.convergence_summary()\n",
    "display(baseline_summary)\n",
    "\n",
    "rhat_ok = (baseline_summary[\"r_hat\"] < 1.01).all()\n",
    "ess_ok = (baseline_summary[\"ess_bulk\"] > 400).all() and (\n",
    "    baseline_summary[\"ess_tail\"] > 400\n",
    ").all()\n",
    "print(f\"\\nR-hat < 1.01 for all parameters: {rhat_ok}\")\n",
    "print(f\"ESS > 400 for all parameters:     {ess_ok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.plot_trace()\n",
    "plt.suptitle(\"Baseline Sales Model — Trace Plots\", y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.plot_energy()\n",
    "plt.suptitle(\"Baseline Sales Model — Energy Diagnostic\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061342c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Posterior Predictive Checks\n",
    "\n",
    "**Why?** If the model fits well, data simulated from the posterior should\n",
    "resemble the observed data. Large discrepancies signal model misspecification.\n",
    "\n",
    "### 4.1 Temperature Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ppc = temp_model.sample_posterior_predictive()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "plot_posterior_predictive(\n",
    "    temp_ppc,\n",
    "    data=temp_model.data,\n",
    "    n_samples=100,\n",
    "    ax=ax,\n",
    "    title=\"Temperature Model — Posterior Predictive Check\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Blue traces = simulated data from posterior; orange = observed.\")\n",
    "print(\"The simulated data should bracket the observations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b49cb3d",
   "metadata": {},
   "source": [
    "### 4.2 Baseline Sales Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_ppc = baseline_model.sample_posterior_predictive()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "plot_posterior_predictive(\n",
    "    baseline_ppc,\n",
    "    data=baseline_model.data,\n",
    "    n_samples=100,\n",
    "    ax=ax,\n",
    "    title=\"Baseline Sales Model — Posterior Predictive Check\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326b885",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Transfer Learning Model\n",
    "\n",
    "Now we build the transfer model: yearly seasonality is transferred from the temperature posterior via `tune_method=\"parametric\"`. We also fit with NUTS so we can run the full diagnostic suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = (\n",
    "    FlatTrend()\n",
    "    + FourierSeasonality(\n",
    "        period=365.25,\n",
    "        series_order=6,\n",
    "        tune_method=\"parametric\",\n",
    "    )\n",
    "    + FourierSeasonality(period=91.31, series_order=4)\n",
    "    + FourierSeasonality(period=30.44, series_order=3)\n",
    "    + FourierSeasonality(period=7, series_order=2)\n",
    ")\n",
    "\n",
    "print(\"Fitting transfer-learning sales model with NUTS...\")\n",
    "transfer_model.fit(\n",
    "    sales_train,\n",
    "    method=\"nuts\",\n",
    "    idata=temp_model.trace,\n",
    "    t_scale_params=temp_model.t_scale_params,\n",
    "    scaler=\"minmax\",\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5acbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence diagnostics for the transfer model\n",
    "transfer_summary = transfer_model.convergence_summary()\n",
    "display(transfer_summary)\n",
    "\n",
    "rhat_ok = (transfer_summary[\"r_hat\"] < 1.01).all()\n",
    "ess_ok = (transfer_summary[\"ess_bulk\"] > 400).all() and (\n",
    "    transfer_summary[\"ess_tail\"] > 400\n",
    ").all()\n",
    "print(f\"\\nR-hat < 1.01 for all parameters: {rhat_ok}\")\n",
    "print(f\"ESS > 400 for all parameters:     {ess_ok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3544054",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.plot_trace()\n",
    "plt.suptitle(\"Transfer Model — Trace Plots\", y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4fa21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.plot_energy()\n",
    "plt.suptitle(\"Transfer Model — Energy Diagnostic\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive check for transfer model\n",
    "transfer_ppc = transfer_model.sample_posterior_predictive()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "plot_posterior_predictive(\n",
    "    transfer_ppc,\n",
    "    data=transfer_model.data,\n",
    "    n_samples=100,\n",
    "    ax=ax,\n",
    "    title=\"Transfer Model — Posterior Predictive Check\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e7d64f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Full Posterior Summaries\n",
    "\n",
    "Instead of reporting single point estimates, we should report **full posterior summaries** including credible intervals (HDI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEMPERATURE MODEL — Full Posterior Summary\")\n",
    "print(\"=\" * 70)\n",
    "display(temp_model.summary())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRANSFER MODEL — Full Posterior Summary\")\n",
    "print(\"=\" * 70)\n",
    "display(transfer_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee59276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior density plots for key parameters\n",
    "print(\"Temperature Model — Posterior Densities:\")\n",
    "temp_model.plot_posterior()\n",
    "plt.suptitle(\"Temperature Model — Posterior Densities\", y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d02360",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transfer Model — Posterior Densities:\")\n",
    "transfer_model.plot_posterior()\n",
    "plt.suptitle(\"Transfer Model — Posterior Densities\", y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa252d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Model Comparison (WAIC / LOO-CV)\n",
    "\n",
    "Information criteria allow principled comparison of models without held-out data. LOO-CV (via PSIS) is generally preferred over WAIC as it is more robust.\n",
    "\n",
    "**Note:** Both models must be fitted with MCMC/VI and the traces must contain log-likelihood values for WAIC/LOO to work. In some pymc versions this is only available when `idata_kwargs={\"log_likelihood\": True}` is passed. Vangja calculates the log-likelihood if it is not already present in the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual WAIC / LOO scores\n",
    "print(\"Baseline model — LOO:\")\n",
    "try:\n",
    "    baseline_loo = baseline_model.loo()\n",
    "    print(baseline_loo)\n",
    "except Exception as e:\n",
    "    print(f\"LOO not available: {e}\")\n",
    "\n",
    "print(\"\\nTransfer model — LOO:\")\n",
    "try:\n",
    "    transfer_loo = transfer_model.loo()\n",
    "    print(transfer_loo)\n",
    "except Exception as e:\n",
    "    print(f\"LOO not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual WAIC scores\n",
    "print(\"Baseline model — WAIC:\")\n",
    "try:\n",
    "    baseline_waic = baseline_model.waic()\n",
    "    print(baseline_waic)\n",
    "except Exception as e:\n",
    "    print(f\"WAIC not available: {e}\")\n",
    "\n",
    "print(\"\\nTransfer model — WAIC:\")\n",
    "try:\n",
    "    transfer_waic = transfer_model.waic()\n",
    "    print(transfer_waic)\n",
    "except Exception as e:\n",
    "    print(f\"WAIC not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf239626",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Prior Sensitivity Analysis\n",
    "\n",
    "A central recommendation of the WAMBS checklist: test whether your conclusions\n",
    "change when you vary the priors. If results are highly sensitive to the prior,\n",
    "you need more data or a better-justified prior.\n",
    "\n",
    "We vary `beta_sd` (the standard deviation of the Fourier coefficient prior) on\n",
    "the **yearly seasonality** in the baseline model. A small `beta_sd` constrains\n",
    "the seasonal amplitude; a large one allows wild oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_baseline(beta_sd=10):\n",
    "    \"\"\"Factory: baseline sales model with variable yearly-seasonality prior.\"\"\"\n",
    "    return (\n",
    "        LinearTrend(n_changepoints=5)\n",
    "        + FourierSeasonality(period=365.25, series_order=6, beta_sd=beta_sd)\n",
    "        + FourierSeasonality(period=91.31, series_order=4)\n",
    "        + FourierSeasonality(period=30.44, series_order=3)\n",
    "        + FourierSeasonality(period=7, series_order=2)\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Running prior sensitivity analysis (this may take a few minutes)...\")\n",
    "sensitivity_results = prior_sensitivity_analysis(\n",
    "    model_factory=make_baseline,\n",
    "    data=sales_train,\n",
    "    param_grid={\"beta_sd\": [0.5, 1, 5, 10, 20, 50]},\n",
    "    fit_kwargs={\"method\": \"mapx\", \"scaler\": \"minmax\"},\n",
    "    metric_data=sales_test,\n",
    "    horizon=len(sales_test),\n",
    ")\n",
    "\n",
    "display(sensitivity_results)\n",
    "print(\"\\nLower RMSE / MAE is better.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c69c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise sensitivity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(\n",
    "    sensitivity_results[\"beta_sd\"], sensitivity_results[\"rmse\"], \"o-\", color=\"C0\"\n",
    ")\n",
    "axes[0].set_xlabel(\"beta_sd (prior std for Fourier coefficients)\")\n",
    "axes[0].set_ylabel(\"Test RMSE\")\n",
    "axes[0].set_title(\"Prior Sensitivity: RMSE\")\n",
    "axes[0].set_xscale(\"log\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(\n",
    "    sensitivity_results[\"beta_sd\"], sensitivity_results[\"mape\"], \"o-\", color=\"C3\"\n",
    ")\n",
    "axes[1].set_xlabel(\"beta_sd (prior std for Fourier coefficients)\")\n",
    "axes[1].set_ylabel(\"Test MAPE (%)\")\n",
    "axes[1].set_title(\"Prior Sensitivity: MAPE\")\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Baseline Model — Prior Sensitivity Analysis\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"If the curve is flat, the results are robust to the prior choice.\")\n",
    "print(\"If RMSE changes dramatically, results are prior-sensitive — report this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af49956a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Prior-to-Posterior Visualisation\n",
    "\n",
    "One of the most intuitive ways to communicate Bayesian results: show how the\n",
    "data **updates** the prior into the posterior.\n",
    "\n",
    "For the temperature model, the Fourier coefficients start from a\n",
    "Normal(0, 10) prior and converge to tight posteriors.\n",
    "\n",
    "For the transfer model, the priors are the *temperature posteriors* —\n",
    "already informative — and the short sales data further refines them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c55f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature model: prior = Normal(0, 10) for Fourier betas\n",
    "temp_prior_params = {\n",
    "    \"ft_0 - intercept\": {\"dist\": \"normal\", \"mu\": 0, \"sigma\": 5},\n",
    "}\n",
    "\n",
    "# Check which variable names exist in the trace\n",
    "print(\"Temperature model variables:\")\n",
    "print(list(temp_model.trace.posterior.data_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f4482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prior_params dict for the Fourier beta coefficients\n",
    "# Default prior for Fourier betas is Normal(0, beta_sd) where beta_sd defaults to 10\n",
    "temp_var_names = [v for v in temp_model.trace.posterior.data_vars if \"beta\" in v]\n",
    "temp_prior_params = {\n",
    "    v: {\"dist\": \"normal\", \"mu\": 0, \"sigma\": 10} for v in temp_var_names\n",
    "}\n",
    "\n",
    "# Also include the intercept\n",
    "intercept_vars = [v for v in temp_model.trace.posterior.data_vars if \"intercept\" in v]\n",
    "for v in intercept_vars:\n",
    "    temp_prior_params[v] = {\"dist\": \"normal\", \"mu\": 0, \"sigma\": 5}\n",
    "\n",
    "print(f\"Plotting prior→posterior for {len(temp_prior_params)} parameters...\")\n",
    "fig = plot_prior_posterior(temp_model.trace, temp_prior_params)\n",
    "fig.suptitle(\"Temperature Model: Prior → Posterior Updating\", fontsize=14, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e370b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer model: prior = Normal(temp_posterior_mean, temp_posterior_std)\n",
    "# These are the transferred posteriors from the temperature model\n",
    "beta_key = \"fs_0 - beta(p=365.25,n=6)\"\n",
    "beta_posterior = temp_model.trace[\"posterior\"][beta_key]\n",
    "beta_mean = beta_posterior.mean(dim=[\"chain\", \"draw\"]).values\n",
    "beta_std = beta_posterior.std(dim=[\"chain\", \"draw\"]).values\n",
    "\n",
    "# Build prior params for the transfer model's Fourier betas\n",
    "# The transfer model's yearly betas have priors = temperature posteriors\n",
    "transfer_beta_vars = [\n",
    "    v for v in transfer_model.trace.posterior.data_vars if \"beta\" in v and \"365\" in v\n",
    "]\n",
    "transfer_prior_params = {}\n",
    "\n",
    "if len(transfer_beta_vars) > 0:\n",
    "    beta_var = transfer_beta_vars[0]\n",
    "    # The beta is a vector; we need individual entries\n",
    "    n_coeffs = transfer_model.trace.posterior[beta_var].shape[-1]\n",
    "    # We'll plot the aggregate distribution\n",
    "    transfer_prior_params[beta_var] = {\n",
    "        \"dist\": \"normal\",\n",
    "        \"mu\": float(beta_mean.mean()),\n",
    "        \"sigma\": float(beta_std.mean()),\n",
    "    }\n",
    "\n",
    "# Also add weekly betas (default priors)\n",
    "weekly_beta_vars = [\n",
    "    v for v in transfer_model.trace.posterior.data_vars if \"beta\" in v and \"7\" in v\n",
    "]\n",
    "for v in weekly_beta_vars:\n",
    "    transfer_prior_params[v] = {\"dist\": \"normal\", \"mu\": 0, \"sigma\": 10}\n",
    "\n",
    "# Add intercept\n",
    "intercept_vars = [\n",
    "    v for v in transfer_model.trace.posterior.data_vars if \"intercept\" in v\n",
    "]\n",
    "for v in intercept_vars:\n",
    "    transfer_prior_params[v] = {\"dist\": \"normal\", \"mu\": 0, \"sigma\": 5}\n",
    "\n",
    "print(f\"Plotting prior→posterior for {len(transfer_prior_params)} parameters...\")\n",
    "print(\"Variables:\", list(transfer_prior_params.keys()))\n",
    "\n",
    "if len(transfer_prior_params) > 0:\n",
    "    fig = plot_prior_posterior(transfer_model.trace, transfer_prior_params)\n",
    "    fig.suptitle(\"Transfer Model: Prior → Posterior Updating\", fontsize=14, y=1.02)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No matching variables found for prior-posterior plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59c6202",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Prediction with Uncertainty\n",
    "\n",
    "A key advantage of Bayesian models: we get a **full predictive distribution**,\n",
    "not just a point estimate. This lets us quantify forecast uncertainty via\n",
    "credible intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fdeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "baseline_pred = baseline_model.predict(horizon=len(sales_test), freq=\"D\")\n",
    "transfer_pred = transfer_model.predict(horizon=len(sales_test), freq=\"D\")\n",
    "\n",
    "# Filter to the relevant date range\n",
    "transfer_pred = transfer_pred[\n",
    "    (transfer_pred[\"ds\"] >= sales_train[\"ds\"].min())\n",
    "    & (transfer_pred[\"ds\"] <= sales_test[\"ds\"].max())\n",
    "]\n",
    "\n",
    "print(f\"Baseline predictions: {baseline_pred.shape[0]} rows\")\n",
    "print(f\"Transfer predictions: {transfer_pred.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side predictions with uncertainty bands\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "for ax, (name, pred, color) in zip(\n",
    "    axes,\n",
    "    [\n",
    "        (\"Baseline (no transfer)\", baseline_pred, \"C3\"),\n",
    "        (\"Transfer Learning\", transfer_pred, \"C2\"),\n",
    "    ],\n",
    "):\n",
    "    # Training data\n",
    "    ax.plot(sales_train[\"ds\"], sales_train[\"y\"], \"C0o\", ms=2, alpha=0.4, label=\"Train\")\n",
    "    # Test data\n",
    "    ax.plot(sales_test[\"ds\"], sales_test[\"y\"], \"C1o\", ms=2, alpha=0.4, label=\"Test\")\n",
    "    # Prediction\n",
    "    ax.plot(pred[\"ds\"], pred[\"yhat_0\"], f\"{color}-\", lw=1.5, label=f\"{name}\")\n",
    "\n",
    "    # Uncertainty bands (if available from MCMC)\n",
    "    if \"yhat_0_lower\" in pred.columns and \"yhat_0_upper\" in pred.columns:\n",
    "        ax.fill_between(\n",
    "            pred[\"ds\"],\n",
    "            pred[\"yhat_0_lower\"],\n",
    "            pred[\"yhat_0_upper\"],\n",
    "            color=color,\n",
    "            alpha=0.15,\n",
    "            label=\"95% credible interval\",\n",
    "        )\n",
    "\n",
    "    ax.axvline(train_test_date, color=\"gray\", ls=\"--\", lw=2)\n",
    "    ax.set_title(name)\n",
    "    ax.set_ylabel(\"Rides\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel(\"Date\")\n",
    "plt.suptitle(\"Predictions with Uncertainty\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca32190",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Quantitative Comparison\n",
    "\n",
    "Finally, we compute forecast metrics on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e807ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_metrics = metrics(sales_test, baseline_pred, pool_type=\"complete\")\n",
    "transfer_metrics = metrics(sales_test, transfer_pred, pool_type=\"complete\")\n",
    "\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Baseline (no transfer)\": baseline_metrics.iloc[0],\n",
    "        \"Transfer Learning\": transfer_metrics.iloc[0],\n",
    "    },\n",
    ").T\n",
    "\n",
    "print(\"Test Set Metrics\")\n",
    "print(\"=\" * 60)\n",
    "display(comparison_df.round(2))\n",
    "\n",
    "# Improvement\n",
    "for col in [\"rmse\", \"mae\", \"mape\"]:\n",
    "    imp = (1 - transfer_metrics[col].values[0] / baseline_metrics[col].values[0]) * 100\n",
    "    print(f\"  {col.upper()} improvement: {imp:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f1023",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Component Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31029deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline model — component decomposition:\")\n",
    "baseline_model.plot(baseline_pred, y_true=sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec7cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transfer model — component decomposition:\")\n",
    "transfer_model.plot(transfer_pred, y_true=sales_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f295a03a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Bayesian Workflow Checklist\n",
    "\n",
    "| Step | Section | Status |\n",
    "|---|---|---|\n",
    "| Prior predictive checks | §2 | ✅ Verified priors produce plausible data |\n",
    "| Convergence diagnostics (R-hat, ESS, BFMI) | §3 | ✅ All models converged |\n",
    "| Posterior predictive checks | §4 | ✅ Fitted models reproduce observed data |\n",
    "| Full posterior summaries (HDI, credible intervals) | §6 | ✅ Reported for all parameters |\n",
    "| Model comparison (LOO / WAIC) | §7 | ✅ Compared baseline vs transfer |\n",
    "| Prior sensitivity analysis | §8 | ✅ Tested beta_sd ∈ {0.5, 1, 5, 10, 20, 50} |\n",
    "| Prior-to-posterior visualisation | §9 | ✅ Showed how data updates beliefs |\n",
    "| Prediction with uncertainty | §10 | ✅ Credible intervals on forecasts |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always run prior predictive checks** before fitting — they catch impossible priors early.\n",
    "2. **Convergence diagnostics are non-negotiable** for MCMC. Check R-hat < 1.01 and ESS > 400.\n",
    "3. **Posterior predictive checks** reveal model misspecification that point metrics miss.\n",
    "4. **Report full posteriors**, not just point estimates — this is the main advantage of Bayesian methods.\n",
    "5. **Test prior sensitivity** — especially for short series where priors have outsized influence.\n",
    "6. **Transfer learning** provides principled, informative priors that make the model robust even with limited data.\n",
    "\n",
    "### References\n",
    "\n",
    "- Kruschke, J. K. (2021). Bayesian Analysis Reporting Guidelines. *Nature Human Behaviour*, 5, 1282–1291.\n",
    "- Vehtari, A., et al. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. *Statistics and Computing*, 27, 1413–1432.\n",
    "- Gabry, J., et al. (2019). Visualization in Bayesian workflow. *JRSS-A*, 182, 389–402."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vangja20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
